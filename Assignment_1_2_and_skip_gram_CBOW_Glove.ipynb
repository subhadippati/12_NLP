{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "Assignment 1: Create a Python application that tokenizes a\n",
        "statement into individual words.\n",
        "Test Case: _\n",
        "Input: \"Hello, how are you?\" _\n",
        "Output: [\"Hello\", \"how\", \"are\", \"you\"]"
      ],
      "metadata": {
        "id": "ca6I9_BVM4gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRrmNEUdBDnl",
        "outputId": "1d562ec7-b197-4091-82f6-3e510ac75ff6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'how', 'are', 'you']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_statement(statement):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    return tokenizer.tokenize(statement)\n",
        "\n",
        "input_text = \"Hello, how are you?\"\n",
        "output = tokenize_statement(input_text)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 2: Implement a Python application using a stemming\n",
        "algorithm (e.g., nltk's PorterStemmer) to reduce words to their\n",
        "root form, and validate it with the test case programming â†’\n",
        "program."
      ],
      "metadata": {
        "id": "UcrMUf65M9qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_word(word):\n",
        "    ps = PorterStemmer()\n",
        "    return ps.stem(word)\n",
        "input_word = \"programming\"\n",
        "output = stem_word(input_word)\n",
        "print(output)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuHhZAOtBLEm",
        "outputId": "62870c42-3ac3-4442-fe56-20ec49426b8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 3: Develop a Python program that utilizes a\n",
        "stemming algorithm, such as the one provided by the nltk library,\n",
        "to efficiently reduce a given word to its lexical root. Validate your\n",
        "implementation by testing it with the input \"running\" and\n",
        "confirming the output as \"run\"."
      ],
      "metadata": {
        "id": "ExAEw0rUM__F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_word(word):\n",
        "    ps = PorterStemmer()\n",
        "    return ps.stem(word)\n",
        "input_word = \"running\"\n",
        "output = stem_word(input_word)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZoo-lzEBOue",
        "outputId": "58cdad53-2602-4bb1-bc7f-af992c339281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 4: Develop a Python program leveraging a\n",
        "lemmatization technique (e.g., nltk's WordNetLemmatizer) to\n",
        "transform a given word into its canonical base form, and validate\n",
        "it using the input \"running\" to produce the output \"run\"."
      ],
      "metadata": {
        "id": "sEBn0umzNCYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def lemmatize_word(word):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return lemmatizer.lemmatize(word, pos=\"v\")\n",
        "print(lemmatize_word(\"running\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SQ2uVa5BQ_j",
        "outputId": "f89fde0b-a5fc-4137-ae2c-24456dada012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 5: Create a Python-based solution to calculate the\n",
        "semantic similarity between two sentences using techniques like\n",
        "cosine similarity or word embeddings, validating it with the input\n",
        "\"This is a sample sentence.\" and \"This sentence is just a sample.\"\n",
        "to produce a similarity score of 0.8."
      ],
      "metadata": {
        "id": "L5ze1vLxNEZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def compute_similarity(sentence1, sentence2):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform([sentence1, sentence2])\n",
        "    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
        "    return round(similarity_score, 2)\n",
        "\n",
        "input_sentence1 = \"This is a sample sentence.\"\n",
        "input_sentence2 = \"This sentence is just a sample.\"\n",
        "output = compute_similarity(input_sentence1, input_sentence2)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHXxE2LJBT7U",
        "outputId": "4bb8b872-c3be-4f54-bd0f-68fe22ba3f3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 6: Develop and implement a Python-based solution\n",
        "to compute the Term Frequency-Inverse Document Frequency\n",
        "(TF-IDF) scores for a small dataset using libraries such as sklearn\n",
        ", validating it with the input {'text': ['This is a sample document.',\n",
        "\n",
        "'Another document with different content.']} to generate the TF-\n",
        "IDF matrix."
      ],
      "metadata": {
        "id": "PGRA751SNHbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_tfidf(data):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(data[\"text\"])\n",
        "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
        "dataset = {\"text\": [\"This is a sample document.\", \"Another document with different content.\"]}\n",
        "tfidf_values, feature_names = compute_tfidf(dataset)\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_values)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wpp0nw0sBdBf",
        "outputId": "95513a6c-2910-443d-ef8e-8ab40fa35ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['another' 'content' 'different' 'document' 'is' 'sample' 'this' 'with']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.37997836 0.53404633 0.53404633\n",
            "  0.53404633 0.        ]\n",
            " [0.47107781 0.47107781 0.47107781 0.33517574 0.         0.\n",
            "  0.         0.47107781]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 7: Create a Python-based text preprocessing\n",
        "program that standardizes input by converting it to lowercase,\n",
        "removing punctuation, and eliminating stopwords using libraries\n",
        "such as nltk or re. Validate the program using the input \"This is a\n",
        "sample text. It contains punctuation and stopwords.\" to produce\n",
        "the output \"sample text contains\"."
      ],
      "metadata": {
        "id": "ALHO4d2eNKim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "text = \"This is a sample text. It contains punctuation and stopwords.\"\n",
        "text = text.lower()\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "words = text.split()\n",
        "filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
        "processed_text = ' '.join(filtered_words)\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4VsDv_rBf9m",
        "outputId": "65235dc5-fc1d-46ac-8e07-b330d88783c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample text contains punctuation stopwords\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Assignment 8: Develop and execute a Python-driven approach\n",
        "to identify named entities within a given sentence, leveraging a\n",
        "named entity recognition module (e.g., nltk's ne_chunk), and\n",
        "validate it using the input \"John Smith works at Google.\" to\n",
        "produce the output [(\"John Smith\", \"PERSON\"), (\"Google\",\n",
        "\"ORGANIZATION\")]."
      ],
      "metadata": {
        "id": "PAQmjRAjNND_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"maxent_ne_chunker\")\n",
        "nltk.download(\"words\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "from nltk.tree import Tree\n",
        "def get_named_entities(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    chunked = ne_chunk(pos_tags)\n",
        "    named_entities = []\n",
        "    for chunk in chunked:\n",
        "        if isinstance(chunk, Tree):\n",
        "            entity_name = \" \".join(token for token, pos in chunk.leaves())\n",
        "            entity_type = chunk.label()\n",
        "            named_entities.append((entity_name, entity_type))\n",
        "\n",
        "    return named_entities\n",
        "sentence = \"John Smith works at Google.\"\n",
        "entities = get_named_entities(sentence)\n",
        "print(entities)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lxg7Xh-SDW_X",
        "outputId": "f642a3f2-394b-48f5-c24f-71c91b8e0045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('John', 'PERSON'), ('Smith', 'PERSON'), ('Google', 'ORGANIZATION')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3\n",
        "\n",
        "1.   Data Preprocessing.\n",
        "2.   Create a Skip-gram model using PyTorch/TensorFlow or NumPy.\n",
        "3.   Modify the Skip-gram model to CBOW.\n",
        "4.   Implement GloVe."
      ],
      "metadata": {
        "id": "E48qhMp6LIAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpmIM6bvK4cH",
        "outputId": "5cab52a2-22f7-40af-d2ad-66d64975059a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import gutenberg\n",
        "import re\n",
        "text = gutenberg.raw('austen-emma.txt')[:50000]\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "tokens = preprocess(text)\n",
        "print(tokens[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wod1WVceLLiW",
        "outputId": "b902663b-fc5c-4f76-f7ec-c93ad6d439c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary Size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Sj0rXqdLOgE",
        "outputId": "afede813-a372-44e3-acdd-08a1e2ca9aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 1761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 2\n",
        "\n",
        "def generate_skipgram_data(tokens, window_size):\n",
        "    skip_grams = []\n",
        "    for i, target in enumerate(tokens):\n",
        "        context_window = tokens[max(i - window_size, 0): i] + tokens[i + 1: i + window_size + 1]\n",
        "        for context in context_window:\n",
        "            skip_grams.append((word_to_idx[target], word_to_idx[context]))\n",
        "    return skip_grams\n",
        "\n",
        "def generate_cbow_data(tokens, window_size):\n",
        "    cbow_data = []\n",
        "    for i in range(window_size, len(tokens) - window_size):\n",
        "        context = tokens[i - window_size:i] + tokens[i + 1:i + window_size + 1]\n",
        "        target = tokens[i]\n",
        "        cbow_data.append(([word_to_idx[word] for word in context], word_to_idx[target]))\n",
        "    return cbow_data\n",
        "\n",
        "skipgram_data = generate_skipgram_data(tokens, window_size)\n",
        "cbow_data = generate_cbow_data(tokens, window_size)\n",
        "\n",
        "print(\"Skip-gram Example:\", skipgram_data[:3])\n",
        "print(\"CBOW Example:\", cbow_data[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St5Ih-OaLQby",
        "outputId": "f98a1c90-c6c9-426f-8de0-c100113b7f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-gram Example: [(471, 207), (471, 836), (207, 471)]\n",
            "CBOW Example: [([471, 207, 122, 1657], 836), ([207, 836, 1657, 764], 122), ([836, 122, 764, 237], 1657)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, center_word):\n",
        "        embed = self.embeddings(center_word)\n",
        "        out = self.linear(embed)\n",
        "        return out\n",
        "\n",
        "model = SkipGram(vocab_size, embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for target, context in random.sample(skipgram_data, 1000):\n",
        "        input_tensor = torch.tensor([target], dtype=torch.long)\n",
        "        context_tensor = torch.tensor([context], dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input_tensor)\n",
        "        loss = criterion(output, context_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"Skip-gram Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RqqbbYHLSzr",
        "outputId": "07158c0f-bcad-4172-a471-24612f513ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skip-gram Epoch: 0 Loss: 7288.345961332321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_words):\n",
        "        embeds = self.embeddings(context_words)\n",
        "        avg_embed = embeds.mean(dim=0).view(1, -1)\n",
        "        out = self.linear(avg_embed)\n",
        "        return out\n",
        "\n",
        "cbow_model = CBOW(vocab_size, embedding_dim)\n",
        "cbow_optimizer = optim.Adam(cbow_model.parameters(), lr=0.001)\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for context, target in random.sample(cbow_data, 1000):\n",
        "        context_tensor = torch.tensor(context, dtype=torch.long)\n",
        "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
        "\n",
        "        cbow_optimizer.zero_grad()\n",
        "        output = cbow_model(context_tensor)\n",
        "        loss = criterion(output, target_tensor)\n",
        "        loss.backward()\n",
        "        cbow_optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(\"CBOW Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9jbOAjjLWe8",
        "outputId": "24309e1c-fa3a-4603-afae-b76d986fcdaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Epoch: 0 Loss: 7167.2594165802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_cooccurrence_matrix(tokens, window_size):\n",
        "    matrix = defaultdict(lambda: defaultdict(int))\n",
        "    for idx, word in enumerate(tokens):\n",
        "        for j in range(max(idx - window_size, 0), min(idx + window_size + 1, len(tokens))):\n",
        "            if idx == j:\n",
        "                continue\n",
        "            matrix[word][tokens[j]] += 1\n",
        "    return matrix\n",
        "\n",
        "co_matrix = build_cooccurrence_matrix(tokens, window_size)\n",
        "\n",
        "X = np.zeros((vocab_size, vocab_size))\n",
        "for w1 in co_matrix:\n",
        "    for w2 in co_matrix[w1]:\n",
        "        i = word_to_idx[w1]\n",
        "        j = word_to_idx[w2]\n",
        "        X[i][j] = co_matrix[w1][w2]\n",
        "\n",
        "embedding_dim = 50\n",
        "W = np.random.rand(vocab_size, embedding_dim)\n",
        "W_context = np.random.rand(vocab_size, embedding_dim)\n",
        "bias = np.random.rand(vocab_size)\n",
        "bias_context = np.random.rand(vocab_size)\n",
        "\n",
        "def glove_loss(i, j, xij):\n",
        "    weight = (xij / 100) ** 0.75 if xij < 100 else 1\n",
        "    dot = np.dot(W[i], W_context[j]) + bias[i] + bias_context[j]\n",
        "    return weight * ((dot - np.log(xij)) ** 2)\n",
        "\n",
        "for epoch in range(1):\n",
        "    total_loss = 0\n",
        "    for i in range(vocab_size):\n",
        "        for j in range(vocab_size):\n",
        "            if X[i][j] > 0:\n",
        "                total_loss += glove_loss(i, j, X[i][j])\n",
        "    print(\"GloVe Epoch:\", epoch, \"Loss:\", total_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AuQVFtDLZXn",
        "outputId": "8505806c-0576-4f0f-f9a3-759dc35105d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe Epoch: 0 Loss: 166903.54231195807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar(word, embeddings, top_n=5):\n",
        "    idx = word_to_idx[word]\n",
        "    vec = embeddings[idx]\n",
        "    sims = np.dot(embeddings, vec)\n",
        "    sorted_idx = np.argsort(-sims)\n",
        "    return [idx_to_word[i] for i in sorted_idx[1:top_n+1]]\n",
        "\n",
        "print(\"Similar to 'emma' in Skip-gram:\", get_similar('emma', model.embeddings.weight.detach().numpy()))\n",
        "print(\"Similar to 'emma' in CBOW:\", get_similar('emma', cbow_model.embeddings.weight.detach().numpy()))\n",
        "print(\"Similar to 'emma' in GloVe:\", get_similar('emma', W))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bc_uetJfLbxA",
        "outputId": "1c5d0a09-9e7d-4c26-bf19-4f8e13635be4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similar to 'emma' in Skip-gram: ['dissuade', 'included', 'widowerfather', 'ill', 'quite']\n",
            "Similar to 'emma' in CBOW: ['sit', 'congratulation', 'an', 'mr', 'over']\n",
            "Similar to 'emma' in GloVe: ['soft', 'widowerfather', 'emma', 'afford', 'prodigies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "News Article Summarizer: Extract the key points from online\n",
        "news articles."
      ],
      "metadata": {
        "id": "jDgcH36oOFjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zfwkG0K_OJVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Job Recommendation System using Resume Matching:\n",
        "Recommend jobs based on resume text similarity.\n",
        "Take a job description dataset from Kaggle.\n",
        "Pre-process job descriptions and resumes.\n",
        "Use TF-IDF and cosine similarity to match resumes to job\n",
        "listings."
      ],
      "metadata": {
        "id": "jGRujHpiOHmf"
      }
    }
  ]
}